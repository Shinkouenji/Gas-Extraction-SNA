{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5670328-1e2d-47a4-b419-8436fcfc44ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas to work with DataFrames\n",
    "import pandas as pd\n",
    "# Import matplotlib for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "# Import json to change the data format\n",
    "import json\n",
    "# Import glob to select multiple files\n",
    "import glob\n",
    "# Import shutil to copy the content of a source file to a destination file\n",
    "import shutil\n",
    "# Import os to mimic operation system functions into python\n",
    "import os\n",
    "# Import datetime to interact with dates as date objects\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf93891f-3141-4c77-b8ea-a98284b577a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the right folder containing the data\n",
    "os. chdir('/scratch/s5724090/TweetData/TwitterGEDv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62ad3ead-0af9-4872-ac9c-796c8f110253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the json data to a DataFrame\n",
    "data_list = []\n",
    "\n",
    "\n",
    "file_list = glob.glob('*.txt')\n",
    "\n",
    "\n",
    "for file in file_list:\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                data_list.append(data)\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Error decoding JSON on line:\", line)\n",
    "\n",
    "\n",
    "groningen_complete = pd.json_normalize(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78ed62a6-1db3-4f17-9ae0-6923e30eac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the necessary columns for social network analysis\n",
    "sna_columns = groningen_complete[['retweeted_status.user.id_str', 'retweeted_status.user.screen_name', 'retweeted_status.user.name', 'retweeted_status.user.description', \n",
    "                                  'user.id_str', 'user.screen_name', 'user.name','user.description' ,'created_at', 'text']]\n",
    "# Rename the columns to make them unambiguous\n",
    "sna_columns = sna_columns.rename(columns={'retweeted_status.user.id_str': 'retweeted_user_id', 'retweeted_status.user.screen_name': 'retweeted_user_handle', \n",
    "                            'retweeted_status.user.name': 'retweeted_user_display_name', 'retweeted_status.user.description':'retweeted_user_bio','user.id_str': 'retweeter_id', 'user.screen_name': 'retweeter_handle', \n",
    "                            'user.name': 'retweeter_display_name','user.description':'retweeter_bio'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29dd196c-74af-4999-a629-d46a232fe3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make datetime objects of the created_at column\n",
    "sna_columns['created_at'] = pd.to_datetime(sna_columns['created_at'])\n",
    "\n",
    "# Format the values to only contain year,  month and day\n",
    "sna_columns['created_at'] = sna_columns['created_at'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Turn them into datetime objects again\n",
    "sna_columns['created_at'] = pd.to_datetime(sna_columns['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "395ef93f-4d56-4928-b48c-61c34b8dddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to make a new dataframe that is filtered on the time period you need based on the first year\n",
    "def filtered_df(dataframe, first_year):\n",
    "    startdate = pd.to_datetime(f'{first_year}-11-16')\n",
    "    enddate = pd.to_datetime(f'{first_year+1}-11-15')\n",
    "    return dataframe.loc[(dataframe['created_at'] >= startdate) & (dataframe['created_at'] <= enddate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83ebcc4c-17cb-4435-9357-29a87b950b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_12_13 = filtered_df(sna_columns, 2012)\n",
    "df_13_14 = filtered_df(sna_columns, 2013)\n",
    "df_14_15 = filtered_df(sna_columns, 2014)\n",
    "df_15_16 = filtered_df(sna_columns, 2015)\n",
    "df_16_17 = filtered_df(sna_columns, 2016)\n",
    "df_17_18 = filtered_df(sna_columns, 2017)\n",
    "df_18_19 = filtered_df(sna_columns, 2018)\n",
    "df_19_20 = filtered_df(sna_columns, 2019)\n",
    "df_20_21 = filtered_df(sna_columns, 2020)\n",
    "df_21_22 = filtered_df(sna_columns, 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2f5dae-80a8-4c06-aa65-0da720d2f4f0",
   "metadata": {},
   "source": [
    "## Making a nodelist and edgelist to use in Gephi\n",
    "#### A nodelist needs to contain a unique node ID, a label and it may contain attributes.\n",
    "#### The unique id's will be the retweeted_user's id and the retweeter_id. The display names will also be necessary in Gephi \n",
    "#### Here, I will make a csv file for the nodes and edges for each year in the network in a format in which it can be \n",
    "#### worked with in Gephi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936027e2-8e51-4f81-84a1-ecef1ee5140f",
   "metadata": {},
   "source": [
    "## Creating the edgelists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3afbffd1-f3cd-4031-ae34-708228a4417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's make a function that also saves both to csv files\n",
    "def create_edgelist(dataframe, filename):\n",
    "    filename = \"\".join([c if c.isalnum() or c in (' ', '.', '_') else \"_\" for c in filename])\n",
    "    # Edgelist\n",
    "    edgelist = dataframe[['retweeter_id','retweeted_user_id']].rename(columns={'retweeter_id':'Source', 'retweeted_user_id':'Target'}).dropna().reset_index().drop(['index'], axis=1)\n",
    "    return edgelist.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18b5a2cd-28c8-4e3d-bf18-e4ec3c959350",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_edgelist(df_12_13, 'gephi_12_13_edges.csv')\n",
    "create_edgelist(df_13_14, 'gephi_13_14_edges.csv')\n",
    "create_edgelist(df_14_15, 'gephi_14_15_edges.csv')\n",
    "create_edgelist(df_15_16, 'gephi_15_16_edges.csv')\n",
    "create_edgelist(df_16_17, 'gephi_16_17_edges.csv')\n",
    "create_edgelist(df_17_18, 'gephi_17_18_edges.csv')\n",
    "create_edgelist(df_18_19, 'gephi_18_19_edges.csv')\n",
    "create_edgelist(df_19_20, 'gephi_19_20_edges.csv')\n",
    "create_edgelist(df_20_21, 'gephi_20_21_edges.csv')\n",
    "create_edgelist(df_21_22, 'gephi_21_22_edges.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483a6e58-48ba-4125-b708-522f6b0f6cbc",
   "metadata": {},
   "source": [
    "## Creating the nodelists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b486b2f-a1b9-4983-94de-d70765efe13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nodelist(dataframe, filename):\n",
    "    nodelist = pd.concat([\n",
    "    dataframe[['retweeter_id','retweeter_display_name']].rename(columns={'retweeter_id': 'Id',\n",
    "                                                                         'retweeter_display_name': 'display name'}),\n",
    "    dataframe[['retweeted_user_id','retweeted_user_display_name']].rename(columns={'retweeted_user_id': 'Id',\n",
    "                                                                                   'retweeted_user_display_name': 'display name'\n",
    "                                                                                  })])\n",
    "    nodelist = nodelist.drop_duplicates().reset_index(drop=True)\n",
    "    return nodelist.to_csv(filename, index=False)\n",
    "    #return nodelist.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "660fc581-b283-4d58-95a0-d1d502bae75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_nodelist(df_12_13, 'gephi_12_13_nodes.csv')\n",
    "create_nodelist(df_13_14, 'gephi_13_14_nodes.csv')\n",
    "create_nodelist(df_14_15, 'gephi_14_15_nodes.csv')\n",
    "create_nodelist(df_15_16, 'gephi_15_16_nodes.csv')\n",
    "create_nodelist(df_16_17, 'gephi_16_17_nodes.csv')\n",
    "create_nodelist(df_17_18, 'gephi_17_18_nodes.csv')\n",
    "create_nodelist(df_18_19, 'gephi_18_19_nodes.csv')\n",
    "create_nodelist(df_19_20, 'gephi_19_20_nodes.csv')\n",
    "create_nodelist(df_20_21, 'gephi_20_21_nodes.csv')\n",
    "create_nodelist(df_21_22, 'gephi_21_22_nodes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a03993-2a3c-4efc-9e94-ecb5c7d09474",
   "metadata": {},
   "source": [
    "## Non-longitudinal network\n",
    "### To appreciate the added value of a longitudinal analysis as opposed to analyzing a singular network that represents 10 years, it is interesting to see what the network looks like had it been one network instead of 10. Thus, we create a nodelist and edgelist for the dataframe before it was split on the basis of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33133ef9-c6d9-4dc5-a42d-ff2751f4effd",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_edgelist(sna_columns, 'gephi_non_split_edges.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fcccb97-92ee-422b-980e-b00b21681180",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_nodelist(sna_columns, 'gephi_non_split_nodes.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
